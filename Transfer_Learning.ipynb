{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torchvision.models as models\n",
    "import torch\n",
    "import os\n",
    "from torchvision.transforms import transforms\n",
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't use the csv since VGG requires data of at least 224x224. So created a new data loader and the data should be downloaded from https://www.kaggle.com/datasets/grassknoted/asl-alphabet?datasetId=23079&sortBy=voteCount&select=asl_alphabet_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28000\n",
      "7000\n"
     ]
    }
   ],
   "source": [
    "batch_size = 20\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize(224),\n",
    "                                transforms.RandomHorizontalFlip(),\n",
    "                                transforms.ToTensor()])\n",
    "\n",
    "data_path = \"./\"\n",
    "dataset_images = datasets.ImageFolder(os.path.join(data_path, 'asl_alphabet_train', 'asl_alphabet_train'), transform=transform)\n",
    "dataset_subset = torch.utils.data.Subset(dataset_images, np.random.choice(len(dataset_images), 35000, replace=False))\n",
    "\n",
    "training_size = int(len(dataset_subset) * 0.8)\n",
    "print(training_size)\n",
    "testing_size = len(dataset_subset) - training_size\n",
    "print(testing_size)\n",
    "\n",
    "image_data_train, image_data_val = torch.utils.data.random_split(dataset_subset, lengths=[training_size, testing_size])\n",
    "\n",
    "data_loader_train = torch.utils.data.DataLoader(image_data_train, batch_size=batch_size, shuffle=True)\n",
    "data_loader_val = torch.utils.data.DataLoader(image_data_val, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "for data, target in data_loader_train:\n",
    "    print(data.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_vgg = models.vgg16(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=29, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_features = model_vgg.classifier[6].in_features\n",
    "model_vgg.classifier[6] = nn.Linear(number_features, 29)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.SGD(model_vgg.parameters(), lr=0.0001, momentum=0.9)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_vgg.to(device)\n",
    "model_vgg.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "for data, target in data_loader_train:\n",
    "    print(data.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our main training loop\n",
    "def train(model, loss_func, epoch, train_dl, test_dl, dev, train_loss, train_acc, valid_loss, valid_acc):\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for data, target in train_dl:\n",
    "        data = data.to(dev)\n",
    "        # target = target.long()\n",
    "        target = target.to(dev)\n",
    "\n",
    "        opt.zero_grad() # zero out gradients (pytorch accumulates by default)\n",
    "        output = model(data)\n",
    "        loss = loss_func(output, target)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        # _, predicted = output.max(1)\n",
    "        pred = output.data.max(1, keepdim=True)[1] # what does the model predict?\n",
    "        correct += pred.eq(target.data.view_as(pred)).sum().item() # was the model correct?\n",
    "\n",
    "    loss = running_loss / len(train_dl.dataset)\n",
    "    acc = 100.*correct / len(train_dl.dataset)\n",
    "    train_loss.append(loss)\n",
    "    train_acc.append(acc)\n",
    "\n",
    "    print(f'Train results for Epoch {epoch+1} --> Loss: {loss} | Accuracy: {acc}')\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    running_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_dl:\n",
    "            data = data.to(dev)\n",
    "            # target = target.long()\n",
    "            target = target.to(dev)\n",
    "\n",
    "            output = model(data)\n",
    "            running_loss += loss_func(output, target).item()\n",
    "            pred = output.data.max(1, keepdim=True)[1] # what does the model predict?\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum().item() # was the model correct?\n",
    "\n",
    "        loss = running_loss / len(test_dl.dataset)\n",
    "        acc = 100*correct / len(test_dl.dataset)\n",
    "        valid_acc.append(acc)\n",
    "        valid_loss.append(loss)\n",
    "        print(f'Test results for Epoch {epoch+1} --> Loss: {loss} | Accuracy: {acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train results for Epoch 1 --> Loss: 0.044892273891550885 | Accuracy: 73.80357142857143\n",
      "Test results for Epoch 1 --> Loss: 0.002747373742916222 | Accuracy: 98.47142857142858\n",
      "Train results for Epoch 2 --> Loss: 0.003704654847554463 | Accuracy: 97.51428571428572\n",
      "Test results for Epoch 2 --> Loss: 0.0006461166549020813 | Accuracy: 99.62857142857143\n",
      "Train results for Epoch 3 --> Loss: 0.0016073512372277038 | Accuracy: 98.96071428571429\n",
      "Test results for Epoch 3 --> Loss: 0.0004854694740611844 | Accuracy: 99.72857142857143\n",
      "Train results for Epoch 4 --> Loss: 0.0009214565634061144 | Accuracy: 99.41428571428571\n",
      "Test results for Epoch 4 --> Loss: 0.00022018042716029284 | Accuracy: 99.87142857142857\n",
      "Train results for Epoch 5 --> Loss: 0.000685555259650008 | Accuracy: 99.58928571428571\n",
      "Test results for Epoch 5 --> Loss: 0.000168340342691928 | Accuracy: 99.85714285714286\n",
      "Train results for Epoch 6 --> Loss: 0.000569553148000458 | Accuracy: 99.61428571428571\n",
      "Test results for Epoch 6 --> Loss: 0.00011037130571906216 | Accuracy: 99.94285714285714\n",
      "Train results for Epoch 7 --> Loss: 0.00038521202887797505 | Accuracy: 99.77857142857142\n",
      "Test results for Epoch 7 --> Loss: 5.10641196648781e-05 | Accuracy: 99.97142857142858\n",
      "Train results for Epoch 8 --> Loss: 0.00033147593380089443 | Accuracy: 99.77857142857142\n",
      "Test results for Epoch 8 --> Loss: 0.00017563876640659665 | Accuracy: 99.88571428571429\n",
      "Train results for Epoch 9 --> Loss: 0.00034445518007303886 | Accuracy: 99.78571428571429\n",
      "Test results for Epoch 9 --> Loss: 4.3789284958291674e-05 | Accuracy: 99.95714285714286\n",
      "Train results for Epoch 10 --> Loss: 0.00016393732410511217 | Accuracy: 99.88214285714285\n",
      "Test results for Epoch 10 --> Loss: 3.754311577565009e-05 | Accuracy: 99.97142857142858\n",
      "Train results for Epoch 11 --> Loss: 0.00022441559867831577 | Accuracy: 99.85714285714286\n",
      "Test results for Epoch 11 --> Loss: 4.588791592593517e-05 | Accuracy: 99.97142857142858\n",
      "Train results for Epoch 12 --> Loss: 0.000178213103787499 | Accuracy: 99.88571428571429\n",
      "Test results for Epoch 12 --> Loss: 2.0946867238413755e-05 | Accuracy: 99.98571428571428\n",
      "Train results for Epoch 13 --> Loss: 0.0001232537965989248 | Accuracy: 99.92857142857143\n",
      "Test results for Epoch 13 --> Loss: 4.8152684855663454e-05 | Accuracy: 99.97142857142858\n",
      "Train results for Epoch 14 --> Loss: 0.00011074053158694078 | Accuracy: 99.94285714285714\n",
      "Test results for Epoch 14 --> Loss: 2.5990630033628496e-05 | Accuracy: 99.98571428571428\n",
      "Train results for Epoch 15 --> Loss: 0.00012701791030610466 | Accuracy: 99.93214285714286\n",
      "Test results for Epoch 15 --> Loss: 3.540094341032329e-05 | Accuracy: 99.98571428571428\n",
      "Train results for Epoch 16 --> Loss: 0.0001538995370460319 | Accuracy: 99.91428571428571\n",
      "Test results for Epoch 16 --> Loss: 2.670096713534268e-05 | Accuracy: 99.98571428571428\n",
      "Train results for Epoch 17 --> Loss: 6.123781603160643e-05 | Accuracy: 99.97142857142858\n",
      "Test results for Epoch 17 --> Loss: 2.4322354616005372e-05 | Accuracy: 99.98571428571428\n",
      "Train results for Epoch 18 --> Loss: 6.922802330731922e-05 | Accuracy: 99.97857142857143\n",
      "Test results for Epoch 18 --> Loss: 1.0243063885634689e-05 | Accuracy: 99.98571428571428\n",
      "Train results for Epoch 19 --> Loss: 9.070929589871669e-05 | Accuracy: 99.94285714285714\n",
      "Test results for Epoch 19 --> Loss: 4.143897451435917e-05 | Accuracy: 99.97142857142858\n",
      "Train results for Epoch 20 --> Loss: 0.00010782674656323182 | Accuracy: 99.93928571428572\n",
      "Test results for Epoch 20 --> Loss: 4.823431537388956e-05 | Accuracy: 99.95714285714286\n",
      "Train results for Epoch 21 --> Loss: 7.041365533034392e-05 | Accuracy: 99.96428571428571\n",
      "Test results for Epoch 21 --> Loss: 4.753120388078815e-05 | Accuracy: 99.94285714285714\n",
      "Train results for Epoch 22 --> Loss: 8.399047248430286e-05 | Accuracy: 99.94285714285714\n",
      "Test results for Epoch 22 --> Loss: 1.7445659869681306e-05 | Accuracy: 99.98571428571428\n",
      "Train results for Epoch 23 --> Loss: 8.734767546989627e-05 | Accuracy: 99.94642857142857\n",
      "Test results for Epoch 23 --> Loss: 3.373049780447143e-05 | Accuracy: 99.97142857142858\n",
      "Train results for Epoch 24 --> Loss: 8.099819628955598e-05 | Accuracy: 99.94642857142857\n",
      "Test results for Epoch 24 --> Loss: 5.817183961513755e-05 | Accuracy: 99.95714285714286\n",
      "Train results for Epoch 25 --> Loss: 7.503967582419005e-05 | Accuracy: 99.94642857142857\n",
      "Test results for Epoch 25 --> Loss: 4.4519216553866715e-05 | Accuracy: 99.98571428571428\n"
     ]
    }
   ],
   "source": [
    "# do the training\n",
    "train_loss, train_acc = [], []\n",
    "valid_loss, valid_acc = [], []\n",
    "\n",
    "loss_arr, acc_arr = [], []\n",
    "for epoch in range(25):\n",
    "    train(model_vgg, loss_function, epoch, data_loader_train, data_loader_val, device, train_loss, train_acc, valid_loss, valid_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_vgg.state_dict(), 'model_vgg2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'training_loss': train_loss, 'training_acc': train_acc, 'valid_loss': valid_loss, 'valid_acc': valid_acc}\n",
    "df = pd.DataFrame(data)\n",
    "df.index.name = 'epoch'\n",
    "df.to_csv('training_data_vgg.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RestNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to C:\\Users\\pierc/.cache\\torch\\hub\\checkpoints\\resnet50-0676ba61.pth\n",
      "100%|██████████| 97.8M/97.8M [00:02<00:00, 47.5MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=29, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet = models.resnet50(pretrained=True)\n",
    "number_features = resnet.fc.in_features\n",
    "resnet.fc = nn.Linear(number_features, 29)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.SGD(resnet.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "resnet.to(device)\n",
    "resnet.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train() missing 1 required positional argument: 'acc_arr'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\pierc\\School Projects\\EE461P\\Final Project\\EE461P_Final_Project\\Transfer_Learning.ipynb Cell 15'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/pierc/School%20Projects/EE461P/Final%20Project/EE461P_Final_Project/Transfer_Learning.ipynb#ch0000014?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/pierc/School%20Projects/EE461P/Final%20Project/EE461P_Final_Project/Transfer_Learning.ipynb#ch0000014?line=2'>3</a>\u001b[0m     loss_arr, acc_arr \u001b[39m=\u001b[39m [], []\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/pierc/School%20Projects/EE461P/Final%20Project/EE461P_Final_Project/Transfer_Learning.ipynb#ch0000014?line=3'>4</a>\u001b[0m     train(resnet, epoch, data_loader_train, data_loader_val, device, loss_arr, acc_arr)\n",
      "\u001b[1;31mTypeError\u001b[0m: train() missing 1 required positional argument: 'acc_arr'"
     ]
    }
   ],
   "source": [
    "# do the training\n",
    "for epoch in range(1):\n",
    "    loss_arr, acc_arr = [], []\n",
    "    train(resnet, epoch, data_loader_train, data_loader_val, device, loss_arr, acc_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
